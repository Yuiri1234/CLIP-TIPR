<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="canonical" href="https://yuiri1234.github.io/CLIP-TIPR/">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Dataset">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets. We propose a method of automatically constructing the LLM Privacy Text Dataset (LPT Dataset), a privacy-related text dataset with privacy indicators, and a method of recognizing whether observing a scene violates privacy without ethically sensitive training images. In constructing the LPT Dataset, which consists of both private and public scenes, we use an LLM to define privacy indicators and generate texts scored for each indicator. Our model recognizes whether a given image is private or public by retrieving texts with privacy scores similar to the image in a multi-modal feature space. In our experiments, we evaluated the performance of our model on three image privacy datasets and a realistic experiment with a humanoid robot in terms of accuracy and responsibility. The experiments show that our approach identifies the private image as accurately as the highly tuned LVLM without delay.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Privacy-Preserving Robotics, Privacy Image Recognition, Low-Latency Privacy-Aware Robot, Privacy Text Dataset Construction, Large Language Model (LLM), Vision Language Model (VLM), CLIP, Privacy, Human-like Behavior, Computer Vision">
  <!-- TODO: List all authors -->
  <meta name="author" content="Yuta Irisawa, Tomoaki Yamazaki, Seiya Ito, Shuhei Kurita, Ryota Akasaka, Masaki Onishi, Kouzou Ohara, Ken Sakurada">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets. We propose a method of automatically constructing the LLM Privacy Text Dataset (LPT Dataset), a privacy-related text dataset with privacy indicators, and a method of recognizing whether observing a scene violates privacy without ethically sensitive training images. In constructing the LPT Dataset, which consists of both private and public scenes, we use an LLM to define privacy indicators and generate texts scored for each indicator. Our model recognizes whether a given image is private or public by retrieving texts with privacy scores similar to the image in a multi-modal feature space. In our experiments, we evaluated the performance of our model on three image privacy datasets and a realistic experiment with a humanoid robot in terms of accuracy and responsibility. The experiments show that our approach identifies the private image as accurately as the highly tuned LVLM without delay.">
　<!-- <meta property="og:url" content="https://www.youtube.com/watch?v=rR9-Z8RhYtQ"> -->
  <meta property="og:image" content="https://yuiri1234.github.io/CLIP-TIPR/static/images/abstract.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="CLIP-TIPR overview">
  
  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets">
  <meta name="citation_author" content="Yuta, Irisawa">
  <meta name="citation_author" content="Tomoaki, Yamazaki">
  <meta name="citation_author" content="Seiya, Ito">
  <meta name="citation_author" content="Shuhei, Kurita">
  <meta name="citation_author" content="Ryota, Akasaka">
  <meta name="citation_author" content="Masaki, Onishi">
  <meta name="citation_author" content="Kouzou, Ohara">
  <meta name="citation_author" content="Ken, Sakurada">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="IROS 2025">
  <meta name="citation_pdf_url" content="https://yuiri1234.github.io/CLIP-TIPR/static/pdfs/Yuta_IR-CLIPTIPR_IROS2025_IEEE_copyright.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets",
    "description": "Humans typically avert their gaze when faced with situations involving another person's privacy, and humanoid robots should exhibit similar behaviors. Various approaches exist for privacy recognition, including an image privacy recognition model and a Large Vision-Language Model (LVLM). The former relies on datasets of labeled images, which raise ethical concerns, while the latter requires more time to recognize images accurately, making real-time responses difficult. To this end, we propose a method of automatically constructing the LLM Privacy Text Dataset (LPT Dataset), a privacy-related text dataset with privacy indicators, and a method of recognizing whether observing a scene violates privacy without ethically sensitive training images. In constructing the LPT Dataset, which consists of both private and public scenes, we use an LLM to define privacy indicators and generate texts scored for each indicator. Our model recognizes whether a given image is private or public by retrieving texts with privacy scores similar to the image in a multi-modal feature space. In our experiments, we evaluated the performance of our model on three image privacy datasets and a realistic experiment with a humanoid robot in terms of accuracy and responsibility. The experiments show that our approach identifies the private image as accurately as the highly tuned LVLM without delay.",
    "datePublished": "2025-10-23",
    "author": [
      {
        "@type": "Person",
        "name": "Yuta Irisawa",
        "affiliation": {
          "@type": "Organization",
          "name": "Aoyama Gakuin University, National Institute of Advanced Industrial Science and Technology (AIST)"
        }
      },
      {
        "@type": "Person",
        "name": "Tomoaki Yamazaki",
        "affiliation": {
          "@type": "Organization",
          "name": "Aoyama Gakuin University"
        }
      },
      {
        "@type": "Person",
        "name": "Seiya Ito",
        "affiliation": {
          "@type": "Organization",
          "name": "National Institute of Information and Communications Technology (NICT)"
        }
      },
      {
        "@type": "Person",
        "name": "Ryota Akasaka",
        "affiliation": {
          "@type": "Organization",
          "name": "Osaka University"
        }
      },
      {
        "@type": "Person",
        "name": "Masaki Onishi",
        "affiliation": {
          "@type": "Organization",
          "name": "National Institute of Advanced Industrial Science and Technology (AIST)"
        }
      },
      {
        "@type": "Person",
        "name": "Shuhei Kurita",
        "affiliation": {
          "@type": "Organization",
          "name": "National Institute of Informatics (NII)"
        }
      },
      {
        "@type": "Person",
        "name": "Kouzou Ohara",
        "affiliation": {
          "@type": "Organization",
          "name": "Aoyama Gakuin University"
        }
      },
      {
        "@type": "Person",
        "name": "Ken Sakurada",
        "affiliation": {
          "@type": "Organization",
          "name": "Kyoto University"
        }
      }
    ],
    "publisher": {
      "@type": "Organization",
      "name": "The 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) "
    },
    "keywords": ["Privacy-Preserving Robotics", "Privacy Image Recognition", "Low-Latency Privacy-Aware Robot", "Privacy Text Dataset Construction", "Large Language Model (LLM)", "Vision Language Model (VLM)", "CLIP", "Privacy", "Human-like Behavior", "Computer Vision"],
    "abstract": "Humans typically avert their gaze when faced with situations involving another person's privacy, and humanoid robots should exhibit similar behaviors. Various approaches exist for privacy recognition, including an image privacy recognition model and a Large Vision-Language Model (LVLM). The former relies on datasets of labeled images, which raise ethical concerns, while the latter requires more time to recognize images accurately, making real-time responses difficult. To this end, we propose a method of automatically constructing the LLM Privacy Text Dataset (LPT Dataset), a privacy-related text dataset with privacy indicators, and a method of recognizing whether observing a scene violates privacy without ethically sensitive training images. In constructing the LPT Dataset, which consists of both private and public scenes, we use an LLM to define privacy indicators and generate texts scored for each indicator. Our model recognizes whether a given image is private or public by retrieving texts with privacy scores similar to the image in a multi-modal feature space. In our experiments, we evaluated the performance of our model on three image privacy datasets and a realistic experiment with a humanoid robot in terms of accuracy and responsibility. The experiments show that our approach identifies the private image as accurately as the highly tuned LVLM without delay."
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">Yuta Irisawa<sup>1,2</sup>,</span>
              <span class="author-block">Tomoaki Yamazaki<sup>1</sup>,</span>
              <span class="author-block">Seiya Ito<sup>3</sup>,</span>
              <span class="author-block">Shuhei Kurita<sup>4</sup>,</span>
              <span class="author-block">Ryota Akasaka<sup>5</sup>,</span>
              <span class="author-block">Masaki Onishi<sup>2</sup>,</span>
              <span class="author-block">Kouzou Ohara<sup>1</sup>,</span>
              <span class="author-block">Ken Sakurada<sup>6</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your institution and conference/journal info -->
              <span class="author-block"><sup>1</sup>Aoyama Gakuin University<br><sup>2</sup>National Institute of Advanced Industrial Science and Technology (AIST)<br><sup>3</sup>National Institute of Information and Communications Technology (NICT)<br><sup>4</sup>National Institute of Informatics (NII)<br><sup>5</sup>Osaka University<br><sup>6</sup>Kyoto University<br>The 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="static/pdfs/Yuta_IR-CLIPTIPR_IROS2025_IEEE_copyright.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Yuiri1234/CLIP-TIPR/tree/master/CLIP-TIPR" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Github (CLIP-TIPR's sample code) </span>
                  </a>
                </span>
              

              <!-- TODO: Add your supplementary material PDF or remove this section -->
              <span class="link-block">
                <a href="https://github.com/Yuiri1234/CLIP-TIPR/blob/master/supplement.md" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Prompts for constructing LPT Dataset (supplement)</span>
                </a>
              </span>

              <!-- TODO: Replace with your GitHub repository URL -->
              <!-- <span class="link-block">
                <a href="https://github.com/YOUR REPO HERE" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
              </span> -->

              <!-- TODO: Update with your arXiv paper ID -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Humans typically avert their gaze when faced with situations involving another person's privacy, and humanoid robots should exhibit similar behaviors. Various approaches exist for privacy recognition, including an image privacy recognition model and a Large Vision-Language Model (LVLM). The former relies on datasets of labeled images, which raise ethical concerns, while the latter requires more time to recognize images accurately, making real-time responses difficult. To this end, we propose a method of automatically constructing the LLM Privacy Text Dataset (LPT Dataset), a privacy-related text dataset with privacy indicators, and a method of recognizing whether observing a scene violates privacy without ethically sensitive training images. In constructing the LPT Dataset, which consists of both private and public scenes, we use an LLM to define privacy indicators and generate texts scored for each indicator. Our model recognizes whether a given image is private or public by retrieving texts with privacy scores similar to the image in a multi-modal feature space. In our experiments, we evaluated the performance of our model on three image privacy datasets and a realistic experiment with a humanoid robot in terms of accuracy and responsibility. The experiments show that our approach identifies the private image as accurately as the highly tuned LVLM without delay.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title video">Overview Video</h2>
      <!-- TODO: Replace with your teaser video -->
      <video poster="" id="tree" autoplay controls muted loop width="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/changing_clothes.mp4" type="video/mp4">
      </video>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        We propose the method that enables the humanoid robot to look away when it accidentally observes privacy-sensitive scenes, such as changing clothes. While the scene continues, the robot alternates between glancing and looking away to confirm the situation. After the scene ends, it resumes normal behavior.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/abstract.png" alt="The proposed framework" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          The proposed framework of CLIP-TIPR.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/LLM_Privacy_Text_Dataset.png" alt="Construction of LLM Privacy Text (LPT) Dataset" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Construction of LLM Privacy Text (LPT) Dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/result.png" alt="Experimental results of LPT Daset and User Feedback" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Experimental results of LPT Daset and User Feedback.
       </h2>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img src="static/images/user_study.png" alt="User study with 200 participants" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         User study with 200 participants.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation (IROS Supplement) </h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- TODO: Replace with your YouTube video ID -->
            <iframe src="https://www.youtube.com/embed/rR9-Z8RhYtQ?si=u-HOdomZgNlUvdX_" title="[IROS 2025] Low-Latency Privacy-AwareRobot Behavior Guided by Automatically Generated Text Datasets" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->
    
<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <!-- TODO: Replace with your poster PDF -->
      <iframe  src="static/pdfs/poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{Irisawa2025,
    author    = {Yuta, Irisawa and Tomoaki, Yamazaki and Seiya, Ito and Shuhei, Kurita and Ryota, Akasaka and Masaki, Onishi and Kouzou, Ohara and Ken, Sakurada},
    title     = {Low-Latency Privacy-Aware Robot Behavior Guided by Automatically Generated Text Datasets},
    journal   = {IROS},
    year      = {2025}
    }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
